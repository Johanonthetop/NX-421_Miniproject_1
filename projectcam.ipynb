{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f10c6ad-0c1e-4805-8ef9-9c201461efe5",
   "metadata": {},
   "source": [
    "### NX-421: Neural signals and signal processing\n",
    "# Miniproject 1 - Variant 3\n",
    "\n",
    "### Camille Dorster, Toufan Kashaev, Johan Bordet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa385f-b6c3-433d-a0c9-31e8a8333b2d",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "The dataset used is a snippet issued from the Human Connectome Project. In particular, this task was adapted from a previously developed one by Buckner and colleagues (Buckner et al. 2011 [2]). Participants are shown visual cues asking them to either tap their left or right fingers, or squeeze their left or right toes, or move their tongue, the goal being to map motor areas. Each block of a movement type lasted ~12 seconds (10 movements), and is preceded by a ~3 second cue. In each of the two runs, there are 13 blocks, with 2 of tongue movements, 4 of hand movements (2 right and 2 left), and 4 of foot movements (2 right and 2 left). In addition, there are 3 15-second fixation blocks per run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021aee-a0ff-4947-b2d4-a66b5936fa0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7451d332-daec-4e17-9397-0c4931f0f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/data\"\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "from nilearn.image import load_img, new_img_like, concat_imgs\n",
    "from nilearn.masking import compute_epi_mask, apply_mask, unmask\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# Utility function used to load json from file name\n",
    "def get_json_from_file(fname):\n",
    "    f = open(fname)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb43e23-45b0-4970-8b07-4aefbdd93e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 22:20:48.017: Failed to load module \"canberra-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "%gui wx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9b894-3fcd-4475-99af-ca4d3dc0d208",
   "metadata": {},
   "source": [
    "### Starting FSLeyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f653ce6-041c-4119-8d46-8bda6b096f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:20:52: Debug: Adding duplicate image handler for 'Windows bitmap file'\n",
      "22:20:52: Debug: Adding duplicate animation handler for '1' type\n",
      "22:20:52: Debug: Adding duplicate animation handler for '2' type\n",
      "22:20:52: Debug: Adding duplicate image handler for 'Windows bitmap file'\n",
      "22:20:52: Debug: Adding duplicate animation handler for '1' type\n",
      "22:20:52: Debug: Adding duplicate animation handler for '2' type\n",
      "\n",
      "(ipykernel_launcher.py:12663): Gtk-CRITICAL **: 22:20:52.427: gtk_window_resize: assertion 'height > 0' failed\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Start FSLeyes\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc283d9e-5b85-4d57-b9d2-e78b7f86dfc6",
   "metadata": {},
   "source": [
    "### Preparing the paths and structure of the data in folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdf1e55-b9f2-4f2a-aace-4ff41e35653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"/home/jovyan/data\"\n",
    "dataset_id = 'mydataset'\n",
    "subject_id = '101410' \n",
    "subject_dir = 'sub-{}'.format(subject_id)\n",
    "\n",
    "bids_root = op.join(sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "preproc_subject_path = op.join(preproc_root, subject_dir)\n",
    "preproc_anat_path = op.join(preproc_root, subject_dir, 'anat')\n",
    "preproc_func_path = op.join(preproc_root, subject_dir, 'func')\n",
    "preproc_fmap_path = op.join(preproc_root, subject_dir, 'fmap')\n",
    "\n",
    "###################\n",
    "# Create folders relevant for preprocessing.\n",
    "###################\n",
    "mkdir_no_exist(sample_path)\n",
    "mkdir_no_exist(bids_root)\n",
    "mkdir_no_exist(deriv_root)\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(preproc_subject_path)\n",
    "mkdir_no_exist(preproc_anat_path)\n",
    "mkdir_no_exist(preproc_func_path)\n",
    "mkdir_no_exist(preproc_fmap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7c2f38-8b9e-4e3d-8992-5024f081b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|mydataset/\n",
      "|--- derivatives/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-101410/\n",
      "|------------ anat/\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-101410/\n",
      "|------ anat/\n",
      "|--------- sub-101410_T1w.nii.gz\n",
      "|------ func/\n",
      "|--------- events_LR.csv\n",
      "|--------- events_RL.csv\n",
      "|--------- sub-101410_tfMRI_MOTOR_LR.nii\n",
      "|--------- sub-101410_tfMRI_MOTOR_RL.nii\n",
      "|--------- task-motor_bold.json\n",
      "|--------- .ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1260e4-9962-484f-8da3-f0ef1a3bb0f0",
   "metadata": {},
   "source": [
    "## Step 1 : skull stripping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c1b34-c999-4711-90da-4b671913abd7",
   "metadata": {},
   "source": [
    "### Applying BET to get the betted brain mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a38a969-f44c-42d7-9100-8411f21c08cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with BET.\n"
     ]
    }
   ],
   "source": [
    "resulting_mask_path = op.join(preproc_anat_path, 'sub-{}_T1w_mask'.format(subject_id))\n",
    "anatomical_path = op.join(bids_root, subject_dir, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id))\n",
    "betted_brain_path = op.join(preproc_anat_path, 'sub-{}_T1w'.format(subject_id))\n",
    "\n",
    "os.system('bet {} {} -m {}'.format(anatomical_path, betted_brain_path, '-R')) # Robust parameter used\n",
    "\n",
    "print(\"Done with BET.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1592d713-0163-43d2-9e91-00d51bfa6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mask\n",
    "fsleyesDisplay.load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb63092-7423-41cb-90fa-3a6438a7675f",
   "metadata": {},
   "source": [
    "### Applying the betted brain mask to the original brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac41fce-120f-488b-8d4a-05c76f3da1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask applied.\n"
     ]
    }
   ],
   "source": [
    "anatomical_path = op.join(bids_root, subject_dir, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id)) # The original brain\n",
    "betted_brain_path = op.join(preproc_anat_path, 'sub-{}_T1w.nii.gz'.format(subject_id)) # The brain without skull is in the derivatives folder\n",
    "resulting_mask_path = op.join(preproc_anat_path, 'sub-{}_T1w_mask'.format(subject_id)) # The mask to use\n",
    "\n",
    "os.system('fslmaths {} -mas {} {}'.format(anatomical_path, resulting_mask_path, betted_brain_path))\n",
    "\n",
    "print(\"Mask applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd25ea1-c0b5-4e65-8b7e-201d2393a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:12663): Gdk-WARNING **: 22:21:32.753: gdkdrawable-x11.c:952 drawable is not a pixmap or window\n"
     ]
    }
   ],
   "source": [
    "# Display the betted brain\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(betted_brain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cd95a-4d6c-4d25-8e96-57c46df2f9f0",
   "metadata": {},
   "source": [
    "## Step 2 : tissue segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a67ed28-d5e6-4674-9b8d-9d9360b3a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_target = betted_brain_path\n",
    "\n",
    "[os.remove(f) for f in glob.glob(op.join(preproc_anat_path, '*fast*'))] # Just to clean the directory in between runs of the cell\n",
    "segmentation_path = op.join(preproc_anat_path, 'sub-101410_T1w_fast')\n",
    "fast(imgs=[fast_target], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e146e99-1bcc-4bf9-9609-803caba1a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the segmented brain\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(betted_brain_path)\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_anat_path,'*pve_0*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_anat_path,'*pve_1*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_anat_path,'*pve_2*'))[0])\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[1]).cmap = 'Red'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[2]).cmap = 'Green'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[3]).cmap = 'Blue'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524e83f-039d-4322-abbe-92499eb911ad",
   "metadata": {},
   "source": [
    "## Step 3 : Variance normalization and concatenation of the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597849a-7292-4819-88d3-f262d1bea321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the betted brain\n",
    "LR_tfMRI_data_path = op.join(bids_root, 'sub-101410', 'func', 'sub-{}_tfMRI_MOTOR_LR'.format(subject_id))\n",
    "RL_tfMRI_data_path = op.join(bids_root, 'sub-101410', 'func', 'sub-{}_tfMRI_MOTOR_RL'.format(subject_id))\n",
    "\n",
    "concat_tfMRI_data_path = op.join(preproc_func_path, 'sub-{}_task-motor_concat_scaled.nii.gz'.format(subject_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22f8fc-7086-4281-8454-33ae0253f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ python -m pip install fmriprep-docke\n",
    "run_files = [\n",
    "    LR_tfMRI_data_path + '.nii',\n",
    "    RL_tfMRI_data_path + '.nii',\n",
    "]\n",
    "\n",
    "# 1) Make a consistent brain mask (intersection across runs for safety)\n",
    "runs = [load_img(r) for r in run_files]\n",
    "mask = compute_epi_mask(imgs[0])\n",
    "for run in runs[1:]:\n",
    "    mask = new_img_like(mask, (mask.get_fdata()>0) & (compute_epi_mask(run).get_fdata()>0))\n",
    "\n",
    "scaled_runs = []\n",
    "for img in imgs:\n",
    "    # 2) mask to get (T, V)\n",
    "    X = apply_mask(img, mask)                # shape (T, V)\n",
    "    # 3) global mean & demean\n",
    "    mu = X.mean()\n",
    "    X = X - mu\n",
    "    # 4) global variance & rescale to unit variance\n",
    "    # ddof=1 for sample variance; add tiny eps for numerical safety\n",
    "    var = X.var(ddof=1)\n",
    "    X = X / np.sqrt(var + 1e-8)\n",
    "    # 5) put back to 4D image with original affine/header\n",
    "    scaled_img = unmask(X, mask)\n",
    "    scaled_runs.append(scaled_img)\n",
    "\n",
    "# 6) concatenate timewise AFTER scaling\n",
    "concat_img = concat_imgs(scaled_runs)  # -> sub-101410_task-motor_concat_scaled.nii.gz (in memory)\n",
    "nib.save(concat_img, concat_tfMRI_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1116fd6-dd12-44e8-ad89-4ccdf3440c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(concat_tfMRI_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de354d7e-8be7-47be-b7e7-ac69ab225052",
   "metadata": {},
   "source": [
    "## Step 4 : motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f434034-9357-4790-adb0-ad5d1b27f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "path_moco_data = op.join(preproc_func_path, 'sub-{}_tfMRI_MOTOR_LR_moco'.format(subject_id))\n",
    "mcflirt(infile=concat_tfMRI_data_path,o=path_moco_data, plots=True, report=True, dof=6, mats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ad7a6-1a5a-4c12-8873-3dbf547f76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(path_original_data)\n",
    "fsleyesDisplay.load(path_moco_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
